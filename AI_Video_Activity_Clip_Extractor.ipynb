{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpt1RyFBEsn+6Hz09vv0rE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NUMAIRn/AI-Video-Activity-Clip-Extractor/blob/main/AI_Video_Activity_Clip_Extractor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers transformers opencv-python Pillow"
      ],
      "metadata": {
        "id": "2Dgau5mtiT9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "090ZM_l4iPNA"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
        "import torch\n",
        "from PIL import Image\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Load pre-trained image captioning model\n",
        "model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "feature_extractor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "\n",
        "# Load sentence-transformer model for similarity matching\n",
        "similarity_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Caption generation settings\n",
        "max_length = 16\n",
        "num_beams = 4\n",
        "gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}\n",
        "\n",
        "# Function to generate captions for frames\n",
        "def predict_step(images):\n",
        "    pixel_values = feature_extractor(images=images, return_tensors=\"pt\").pixel_values\n",
        "    pixel_values = pixel_values.to(device)\n",
        "    output_ids = model.generate(pixel_values, **gen_kwargs)\n",
        "    preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    return preds\n",
        "\n",
        "# Function to process video, generate captions, and save to a text file\n",
        "def generate_captions(video_path, captions_file):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    frame_count = 0\n",
        "    images_to_caption = []\n",
        "    timestamps = []\n",
        "\n",
        "    # Process the video frame by frame to capture the first frame of each second\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Capture first frame of each second\n",
        "        if frame_count % fps == 0:\n",
        "            # Convert frame (numpy array) to PIL image\n",
        "            pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "            images_to_caption.append(pil_image)\n",
        "            timestamps.append(frame_count // fps)\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "    # Generate captions for the selected frames\n",
        "    captions = predict_step(images_to_caption)\n",
        "\n",
        "    # Save captions to a text file with timestamps\n",
        "    with open(captions_file, 'w') as f:\n",
        "        for caption, timestamp in zip(captions, timestamps):\n",
        "            f.write(f\"{caption}: {timestamp} sec\\n\")\n",
        "\n",
        "    cap.release()\n",
        "    return captions, timestamps\n",
        "\n",
        "# Function to extract part of the video based on the query using semantic similarity\n",
        "def extract_video_by_query(video_path, captions_file, query, output_path, threshold=0.5, max_gap=10):\n",
        "    # Load the captions and timestamps from the text file\n",
        "    captions = []\n",
        "    timestamps = []\n",
        "    with open(captions_file, 'r') as f:\n",
        "        for line in f:\n",
        "            caption, timestamp = line.rsplit(':', 1)\n",
        "            captions.append(caption.strip())\n",
        "            timestamps.append(int(timestamp.strip().replace(\"sec\", \"\")))\n",
        "\n",
        "    # Generate embeddings for the captions\n",
        "    caption_embeddings = similarity_model.encode(captions, convert_to_tensor=True)\n",
        "\n",
        "    # Generate embedding for the query\n",
        "    query_embedding = similarity_model.encode(query, convert_to_tensor=True)\n",
        "\n",
        "    # Calculate cosine similarities between the query and captions\n",
        "    similarities = util.pytorch_cos_sim(query_embedding, caption_embeddings).squeeze()\n",
        "\n",
        "    # Find the timestamps of captions that are most similar to the query\n",
        "    matched_timestamps = []\n",
        "    for idx, similarity in enumerate(similarities):\n",
        "        if similarity.item() > threshold:  # Only consider captions with high similarity\n",
        "            matched_timestamps.append(timestamps[idx])\n",
        "\n",
        "    if not matched_timestamps:\n",
        "        print(f\"No relevant matches found for query: {query}\")\n",
        "        return\n",
        "\n",
        "    # Step 1: Sort matched timestamps\n",
        "    matched_timestamps = sorted(matched_timestamps)\n",
        "\n",
        "    # Step 2: Group the timestamps that are close to each other based on max_gap (e.g., 3 seconds)\n",
        "    grouped_timestamps = []\n",
        "    group = [matched_timestamps[0]]\n",
        "\n",
        "    for i in range(1, len(matched_timestamps)):\n",
        "        if matched_timestamps[i] - matched_timestamps[i-1] <= max_gap:\n",
        "            group.append(matched_timestamps[i])\n",
        "        else:\n",
        "            grouped_timestamps.append(group)\n",
        "            group = [matched_timestamps[i]]\n",
        "\n",
        "    grouped_timestamps.append(group)  # Add the last group\n",
        "\n",
        "    # Step 3: Choose the first group (closest match)\n",
        "    best_group = grouped_timestamps[0]\n",
        "    start_time = min(best_group)\n",
        "    end_time = max(best_group)\n",
        "\n",
        "    # Step 4: Extract video based on start and end times\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    # Define the codec and create VideoWriter object for the output video\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "    frame_count = 0\n",
        "    start_frame = start_time * fps\n",
        "    end_frame = (end_time + 1) * fps  # Include all frames up to the end of the last second\n",
        "\n",
        "    # Extract frames within the time range\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret or frame_count > end_frame:\n",
        "            break\n",
        "\n",
        "        if start_frame <= frame_count <= end_frame:\n",
        "            out.write(frame)\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "def process_video_and_extract(video_path, query, output_video_path, captions_file='captions.txt', similarity_threshold=0.5):\n",
        "    # Step 1: Generate captions and save to file\n",
        "    captions, timestamps = generate_captions(video_path, captions_file)\n",
        "\n",
        "    # Step 2: Extract video based on query using semantic similarity\n",
        "    extract_video_by_query(video_path, captions_file, query, output_video_path, threshold=similarity_threshold)\n",
        "\n",
        "process_video_and_extract(\n",
        "    video_path='input_video_filepath.mp4',\n",
        "    query='user query for video extraction',\n",
        "    output_video_path='extracted_video.mp4',\n",
        "    similarity_threshold=0.5  # Adjust this to control how close the match should be\n",
        ")\n"
      ]
    }
  ]
}